# FastAgent Configuration File

# Default Model Configuration:
#
# Takes format:
#   <provider>.<model_string>.<reasoning_effort?> (e.g. anthropic.claude-3-5-sonnet-20241022 or openai.o3-mini.low)
# Accepts aliases for Anthropic Models: haiku, haiku3, sonnet, sonnet35, opus, opus3
# and OpenAI Models: gpt-4o-mini, gpt-4o, o1, o1-mini, o3-mini
#
# If not specified, defaults to "haiku".
# Can be overriden with a command line switch --model=<model>, or within the Agent constructor.

default_model: passthrough

# Logging and Console Configuration:
logger:
  # level: "debug" | "info" | "warning" | "error"
  # type: "none" | "console" | "file" | "http"
  # path: "/path/to/logfile.jsonl"

  # Switch the progress display on or off
  progress_display: true

  # Show chat User/Assistant messages on the console
  show_chat: true
  # Show tool calls on the console
  show_tools: true
  # Truncate long tool responses on the console
  truncate_tools: true

# MCP Servers
mcp:
  servers:
    prompts:
      command: "prompt-server"
      args: ["playback.md"]
    prompts2:
      command: "prompt-server"
      args: ["prompt.txt"]
    std_io:
      command: "uv"
      args: ["run", "integration_agent.py", "--server", "--transport", "stdio"]
    sse:
      transport: "sse"
      url: "http://localhost:8723/sse"
    http:
      transport: "http"
      url: "http://localhost:8724/mcp"
    card_test:
      command: "uv"
      args: ["run", "mcp_tools_server.py"]
    hyphen-test:
      command: "uv"
      args: ["run", "mcp_tools_server.py"]
    # borrows config from prompt-server
    cwd_test:
      command: "prompt-server"
      args: ["multi.txt"]
      cwd: "../prompt-server/"
    dynamic_tool:
      command: "uv"
      args: ["run", "mcp_dynamic_tools.py"]
